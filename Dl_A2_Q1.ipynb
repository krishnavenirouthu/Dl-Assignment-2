{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1m39lqZxXYFd-rOnxjinN7ZKk4bpvMyw3",
      "authorship_tag": "ABX9TyOZGuBOKCc2ti/usPGEms5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnavenirouthu/Dl-Assignment-2/blob/main/Dl_A2_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LwIjClIx6pz1",
        "outputId": "4b314578-73a0-44d4-867a-50a8573a7092"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch==2.0.1\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp311-cp311-linux_x86_64.whl (195.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.21.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.19.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.19.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.18.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.18.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.16.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.16.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.16.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.15.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.3.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.1.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.1.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.1.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.0.2%2Bcpu-cp311-cp311-linux_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.0.1+cpu torchaudio-2.0.2+cpu torchvision-0.15.2+cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchgen"
                ]
              },
              "id": "30ae8981d6154ffc98d605d138f3b20d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---------- Device ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- Data Loading ----------\n",
        "def read_tsv(path):\n",
        "    data = pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "    data = data.dropna()\n",
        "    data.columns = [\"target\", \"source\", \"freq\"]\n",
        "    return [(str(row[\"source\"]), str(row[\"target\"])) for _, row in data.iterrows()]\n",
        "\n",
        "# ---------- Vocabulary ----------\n",
        "class CharVocab:\n",
        "    def __init__(self, sequences, specials=[\"<pad>\", \"<sos>\", \"<eos>\"]):\n",
        "        chars = sorted(set(\"\".join(sequences)))\n",
        "        self.itos = specials + chars\n",
        "        self.stoi = {ch: i for i, ch in enumerate(self.itos)}\n",
        "\n",
        "    def encode(self, sequence):\n",
        "        return [self.stoi[\"<sos>\"]] + [self.stoi[c] for c in sequence] + [self.stoi[\"<eos>\"]]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        chars = [self.itos[i] for i in indices]\n",
        "        return \"\".join([c for c in chars if c not in [\"<sos>\", \"<eos>\", \"<pad>\"]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, data, src_vocab, tgt_vocab):\n",
        "        self.data = data\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        return self.src_vocab.encode(src), self.tgt_vocab.encode(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_seqs, tgt_seqs = zip(*batch)\n",
        "    src_lens = [len(s) for s in src_seqs]\n",
        "    tgt_lens = [len(t) for t in tgt_seqs]\n",
        "    max_src = max(src_lens)\n",
        "    max_tgt = max(tgt_lens)\n",
        "    pad_idx = 0\n",
        "\n",
        "    src_padded = [s + [pad_idx] * (max_src - len(s)) for s in src_seqs]\n",
        "    tgt_padded = [t + [pad_idx] * (max_tgt - len(t)) for t in tgt_seqs]\n",
        "\n",
        "    return (\n",
        "        torch.tensor(src_padded, device=device),\n",
        "        torch.tensor(tgt_padded, device=device),\n",
        "        torch.tensor(src_lens, device=device),\n",
        "        torch.tensor(tgt_lens, device=device),\n",
        "    )\n",
        "\n",
        "# ---------- Encoder ----------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout=0.3, cell_type=\"lstm\"):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        rnn_cls = {\"rnn\": nn.RNN, \"lstm\": nn.LSTM, \"gru\": nn.GRU}[cell_type]\n",
        "        self.rnn = rnn_cls(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        embedded = self.embedding(src)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        outputs, hidden = self.rnn(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        return hidden\n",
        "\n",
        "# ---------- Decoder ----------\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout=0.3, cell_type=\"lstm\"):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
        "        rnn_cls = {\"rnn\": nn.RNN, \"lstm\": nn.LSTM, \"gru\": nn.GRU}[cell_type]\n",
        "        self.rnn = rnn_cls(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "# ---------- Seq2Seq ----------\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim=128, hid_dim=256, n_layers=2, dropout=0.3, cell_type=\"lstm\"):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, emb_dim, hid_dim, n_layers, dropout, cell_type)\n",
        "        self.decoder = Decoder(tgt_vocab_size, emb_dim, hid_dim, n_layers, dropout, cell_type)\n",
        "\n",
        "    def forward(self, src, src_len, tgt, teacher_forcing_ratio=0.7):\n",
        "        if not self.training:\n",
        "            teacher_forcing_ratio = 0.0\n",
        "\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n",
        "        hidden = self.encoder(src, src_len)\n",
        "\n",
        "        input = tgt[:, 0]\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# ---------- Training ----------\n",
        "def train_model(model, train_loader, dev_loader, optimizer, loss_fn, epochs=30):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for src, tgt, src_len, tgt_len in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, src_len, tgt)\n",
        "            output_flat = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "            tgt_flat = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = loss_fn(output_flat, tgt_flat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = output_flat.argmax(1)\n",
        "            mask = tgt_flat != 0\n",
        "            total_correct += (pred == tgt_flat).masked_select(mask).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "        acc = 100 * total_correct / total_tokens\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "    print(\"\\nEvaluating on Dev Set:\")\n",
        "    evaluate_model(model, dev_loader, loss_fn)\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "def evaluate_model(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt, src_len, tgt_len in loader:\n",
        "            output = model(src, src_len, tgt)\n",
        "            output_flat = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "            tgt_flat = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = loss_fn(output_flat, tgt_flat)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = output_flat.argmax(1)\n",
        "            mask = tgt_flat != 0\n",
        "            total_correct += (pred == tgt_flat).masked_select(mask).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "    acc = 100 * total_correct / total_tokens\n",
        "    print(f\"Test Loss = {total_loss/len(loader):.4f}, Test Accuracy = {acc:.2f}%\")\n",
        "\n",
        "# ---------- Inference ----------\n",
        "def transliterate(model, word, src_vocab, tgt_vocab, max_len=30):\n",
        "    model.eval()\n",
        "    src_encoded = torch.tensor([src_vocab.encode(word)], device=device)\n",
        "    src_len = torch.tensor([len(src_encoded[0])], device=device)\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(src_encoded, src_len)\n",
        "        input = torch.tensor([tgt_vocab.stoi[\"<sos>\"]], device=device)\n",
        "        result = []\n",
        "        for _ in range(max_len):\n",
        "            output, hidden = model.decoder(input, hidden)\n",
        "            top1 = output.argmax(1).item()\n",
        "            if top1 == tgt_vocab.stoi[\"<eos>\"]:\n",
        "                break\n",
        "            result.append(top1)\n",
        "            input = torch.tensor([top1], device=device)\n",
        "    return tgt_vocab.decode(result)\n",
        "\n",
        "# ---------- Main ----------\n",
        "if __name__ == \"__main__\":\n",
        "    train_data = read_tsv(\"/content/drive/MyDrive/te.translit.sampled.train.tsv\")\n",
        "    dev_data = read_tsv(\"/content/drive/MyDrive/te.translit.sampled.dev.tsv\")\n",
        "    test_data = read_tsv(\"/content/drive/MyDrive/te.translit.sampled.test.tsv\")\n",
        "\n",
        "    src_vocab = CharVocab([src for src, _ in train_data])\n",
        "    tgt_vocab = CharVocab([tgt for _, tgt in train_data])\n",
        "\n",
        "    train_ds = TransliterationDataset(train_data, src_vocab, tgt_vocab)\n",
        "    dev_ds = TransliterationDataset(dev_data, src_vocab, tgt_vocab)\n",
        "    test_ds = TransliterationDataset(test_data, src_vocab, tgt_vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "    dev_loader = DataLoader(dev_ds, batch_size=128, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=128, collate_fn=collate_fn)\n",
        "\n",
        "    model = Seq2Seq(len(src_vocab), len(tgt_vocab), emb_dim=128, hid_dim=256, n_layers=2, dropout=0.3, cell_type=\"lstm\").to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    train_model(model, train_loader, dev_loader, optimizer, loss_fn, epochs=30)\n",
        "\n",
        "    print(\"\\nEvaluating on Test Set:\")\n",
        "    evaluate_model(model, test_loader, loss_fn)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for src, tgt in test_data[:5]:\n",
        "        prediction = transliterate(model, src, src_vocab, tgt_vocab)\n",
        "        print(f\"Latin: {src} => Predicted Telugu: {prediction} | Actual: {tgt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKQwmsKX7aTE",
        "outputId": "8449ada2-2b68-4c9b-cf31-3257c75ae24b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 1.8715, Accuracy = 49.28%\n",
            "Epoch 2: Loss = 0.6288, Accuracy = 82.10%\n",
            "Epoch 3: Loss = 0.4103, Accuracy = 88.44%\n",
            "Epoch 4: Loss = 0.3138, Accuracy = 91.28%\n",
            "Epoch 5: Loss = 0.2513, Accuracy = 93.04%\n",
            "Epoch 6: Loss = 0.2125, Accuracy = 94.17%\n",
            "Epoch 7: Loss = 0.1790, Accuracy = 95.06%\n",
            "Epoch 8: Loss = 0.1548, Accuracy = 95.74%\n",
            "Epoch 9: Loss = 0.1373, Accuracy = 96.28%\n",
            "Epoch 10: Loss = 0.1210, Accuracy = 96.72%\n",
            "Epoch 11: Loss = 0.1092, Accuracy = 97.04%\n",
            "Epoch 12: Loss = 0.0990, Accuracy = 97.32%\n",
            "Epoch 13: Loss = 0.0882, Accuracy = 97.58%\n",
            "Epoch 14: Loss = 0.0823, Accuracy = 97.71%\n",
            "Epoch 15: Loss = 0.0750, Accuracy = 97.94%\n",
            "Epoch 16: Loss = 0.0715, Accuracy = 98.01%\n",
            "Epoch 17: Loss = 0.0678, Accuracy = 98.10%\n",
            "Epoch 18: Loss = 0.0619, Accuracy = 98.29%\n",
            "Epoch 19: Loss = 0.0590, Accuracy = 98.35%\n",
            "Epoch 20: Loss = 0.0545, Accuracy = 98.47%\n",
            "Epoch 21: Loss = 0.0532, Accuracy = 98.49%\n",
            "Epoch 22: Loss = 0.0522, Accuracy = 98.52%\n",
            "Epoch 23: Loss = 0.0487, Accuracy = 98.63%\n",
            "Epoch 24: Loss = 0.0471, Accuracy = 98.69%\n",
            "Epoch 25: Loss = 0.0456, Accuracy = 98.71%\n",
            "Epoch 26: Loss = 0.0447, Accuracy = 98.73%\n",
            "Epoch 27: Loss = 0.0449, Accuracy = 98.74%\n",
            "Epoch 28: Loss = 0.0393, Accuracy = 98.87%\n",
            "Epoch 29: Loss = 0.0415, Accuracy = 98.82%\n",
            "Epoch 30: Loss = 0.0415, Accuracy = 98.82%\n",
            "\n",
            "Evaluating on Dev Set:\n",
            "Test Loss = 1.0954, Test Accuracy = 84.68%\n",
            "\n",
            "Evaluating on Test Set:\n",
            "Test Loss = 1.1707, Test Accuracy = 83.72%\n",
            "\n",
            "Sample Predictions:\n",
            "Latin: amkamlo => Predicted Telugu: అంకంలో | Actual: అంకంలో\n",
            "Latin: ankamlo => Predicted Telugu: అంకంలో | Actual: అంకంలో\n",
            "Latin: ankamloo => Predicted Telugu: అంకంలో | Actual: అంకంలో\n",
            "Latin: amkitamai => Predicted Telugu: అంకితమై | Actual: అంకితమై\n",
            "Latin: ankitamai => Predicted Telugu: అంకితమై | Actual: అంకితమై\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "94LXs15k8rQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}